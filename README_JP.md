# CMU-VLA-Challenge

## 目次
[はじめに](#はじめに)  
[目的](#目的)  
[タスク仕様](#タスク仕様)

[セットアップ](#セットアップ)
- [チャレンジシーン](#チャレンジシーン)
- [チャレンジ質問](#チャレンジ質問)
- [システム](#システム)
- [シミュレータ](#シミュレータ)
- [オブジェクト参照データセット](#オブジェクト参照データセット-vla-3d)

[実ロボットチャレンジ](#実ロボットチャレンジ-2025)
- [実ロボットデータ](#実ロボットデータ)

[提出](#提出)

[評価](#評価)
- [質問タイプと初期スコアリング](#質問タイプと初期スコアリング)
- [タイミング](#タイミング)

[チャレンジFAQ](#チャレンジfaq)

## はじめに
CMU Vision-Language-Autonomy Challengeは、ナビゲーション自律性におけるコンピュータビジョンと自然言語理解を活用します。本チャレンジは実環境と実ロボット上での具現化されたAIの限界を押し広げることを目的としており、すべての参加者の成果を実世界での展開により近づけるために、ロボットプラットフォームと動作する自律システムを提供します。本チャレンジは、3D LiDARと360度カメラを搭載した実ロボットシステムを提供します。システムには、センサーポーズの推定、地形の分析、衝突回避、ウェイポイントへのナビゲーションが可能なベース自律性が搭載されています。チームは、システムとインターフェースを持ち、ロボットをナビゲートするためのソフトウェアをロボットのオンボードコンピューターにセットアップします。2024年のチャレンジはカスタムシミュレーション環境で実施され、翌年には実ロボットシステムに移行します。

チャレンジへの登録については、[チャレンジウェブサイト](https://www.ai-meets-autonomy.com/cmu-vla-challenge)をご覧ください。


## 目的 
チームは、自然言語ナビゲーションクエリを受け取り、クエリに基づいてウェイポイントまたは経路を生成することで車両システムをナビゲートできるビジョン・ランゲージモデルを開発することが期待されています。


## タスク仕様
本チャレンジでは、Unity [1]のシーンに対する自然言語の質問/文のセットがチームに提供されます。チームは、システムから提供されるシーンのオンボードデータとともに質問を処理するソフトウェアの開発を担当します。質問/文はすべて、シーン内のオブジェクトの意味的空間理解を必要とする空間推論コンポーネントを含んでいます。環境は最初は未知であり、適切な視点にナビゲートし、システムにウェイポイントを送信してシーンを探索することでシーンデータが収集されます。15のUnityシーンそれぞれに5つの質問/文が提供され、3つのシーンがテスト評価のために保持されます。

自然言語の質問は、数値、オブジェクト参照、指示追従の3つのカテゴリに分かれており、以下で詳しく説明します。

**数値**

数値質問は、特定の属性や空間関係に適合するオブジェクトの数量について尋ねます。応答は整数であることが期待されます。

例：

    テーブルと壁の間にある青い椅子は何脚ありますか？

    窓の近くにある黒いゴミ箱は何個ありますか？

**オブジェクト参照**

オブジェクト参照文は、空間関係および/または属性によって参照されるシーン内の特定のオブジェクトを見つけるようシステムに求めます。応答はオブジェクトの周りのバウンディングボックスであることが期待され、シーン内には正解が1つだけ存在します（参照されるオブジェクトは一意です）。バウンディングボックスマーカーの中心点は、ロボットシステムをナビゲートするためのウェイポイントとして使用されます。

例：

    冷蔵庫に最も近いキッチンアイランドの上にある鉢植えの植物を見つけてください。

    窓に最も近い、テーブルとシンクの間にあるオレンジ色の椅子を見つけてください。

**指示追従**

指示追従文は、オブジェクトを使用して経路の軌跡を指定し、システムに特定の経路を取るように求めます。応答はウェイポイントのシーケンスであることが期待されます。

例：

    窓の近くを通って冷蔵庫への道を進んでください。

    2つのテーブルの間の道を避けて、窓の近くにある青いゴミ箱の近くに行ってください。


## セットアップ
まず、このリポジトリをクローンし、ローカルの `/home/$USER/` フォルダに配置してください。

### チャレンジシーン
チャレンジには合計18のUnityシーンが使用されます。15のシーンがモデル開発用に提供され、3つがテスト用に保持されます。これらのシーンの大部分は単一の部屋で、いくつかは複数の部屋を持つ建物です。トレーニング環境モデルのセットは[こちら](https://drive.google.com/drive/folders/1bmxdT6Oxzt0_0tohye2br7gqTnkMaq20?usp=share_link)からダウンロードできます。15のトレーニングシーンすべてについて、処理されたシーンのポイントクラウド、色とサイズ属性を含むオブジェクトと領域情報、参照言語文も提供しています（詳細は[オブジェクト参照データセット](#オブジェクト参照データセット-vla-3d)をご覧ください）。

![image](figures/scenes.png)

### チャレンジ質問
各Unityシーンのチャレンジ質問のセットは、[questions](questions/)フォルダ下の15のトレーニングシーンそれぞれのPDFファイルで提供されています。各シーンの正解の画像も視覚化のために提供され、ターゲット軌跡の.plyファイルも提供されています。すべてのトレーニングシーンのすべての質問は、[questions/questions.json](questions/questions.json)でJSON形式でも見つけることができます。

### システム

私たちのシステムはUbuntu 20.04で動作し、シミュレーションと実ロボットの両方でROS Noeticを使用します。[docker/](docker/)フォルダの指示に従って、提供されたDockerイメージをプルしてシステムを起動することで、シミュレータを試してください。

システムはデフォルトでUnity環境を使用し、2つの部分から構成されています：
- ベースナビゲーションシステムは[system/unity](system/unity/)フォルダにあります。このシステムは、AIモジュールを実行せずに単独で起動できます。ベースナビゲーションシステムでは、[system/unity/src/vehicle_simulator/mesh/unity/](system/unity/src/vehicle_simulator/mesh/unity/)ディレクトリに配置することで、使用するシーンを変更できます。
- ビジョン・ランゲージモデルは[ai_module](ai_module/)フォルダにあるべきです。現在[ai_module/src](ai_module/src)フォルダ内にあるモデルは、入力質問を読み取る方法を示し、システムと評価ノードで使用されるさまざまなタイプの出力応答の任意の例を生成する「ダミーモデル」です。**チームはこれを置き換えるモデルを開発することが期待されています。**

システム起動スクリプト[launch.sh](launch.sh)を起動すると、デフォルトでUnityシミュレータとダミーモデルの両方が起動されます。

#### ダミーモデル

ダミーモデルは、`/challenge_question`トピックでROS String メッセージとして質問を読み取ります。ダミーモデルは、Int32メッセージとして整数を公開するか、オブジェクト参照用のバウンディングボックス可視化マーカーを送信するか、車両ナビゲーションをガイドするためのウェイポイントを送信します。3種類のメッセージを以下に示します。モデルをシステムと統合するには、システム起動スクリプトを変更してください。
- 数値応答：`/numerical_response`トピックのROS Int32メッセージ、数値質問に答える整数を含む
- 可視化マーカー：`/selected_object_marker`トピックのROS Markerメッセージ、選択されたオブジェクトのオブジェクトラベルとバウンディングボックスを含む
- ウェイポイント：`/way_point_with_heading`トピックのROS Pose2Dメッセージ（今年のチャレンジではヘディングは無視）

#### システム出力
システムは、以下の表に示すようにAIモジュールにオンボードデータを提供します：

| メッセージ | 説明 | 頻度 | フレーム | ROSトピック名 |
|-|-|-|-|-|
| Image | 360度カメラからのROS Imageメッセージ。画像は1920/640解像度で、360度HFOV、120 VFOV | 10Hz | camera | `/camera/image` |
| Registered Scan | 3D LiDARからのROS PointCloud2メッセージで、状態推定モジュールによって登録済み | 5Hz | map | `/registered_scan` |
| Sensor Scan | 3D LiDARからのROS PointCloud2メッセージ | 5Hz | sensor_at_scan | `/sensor_scan` |
| Local Terrain Map | 車両周辺の地形分析モジュールからのROS PointCloud2メッセージ | 5Hz | map | `/terrain_map` (車両周辺5m) <br> `/terrain_map_ext` (車両周辺20m) |
| Sensor Pose| 状態推定モジュールからのROS Odometryメッセージ | 100-200Hz | mapからsensorへ | `/state_estimation` |
| Traversable Area| 環境全体の通行可能領域を含むROS PointCloud2メッセージ | 5Hz | map | `/traversable_area` |
| Ground-Truth Semantics| 車両周辺2m以内のオブジェクトラベルとバウンディングボックスを含むROS MarkerArrayメッセージ | 5Hz | map | `/object_markers` |

**重要な注意事項**：システムからより多くのトピックが利用可能かもしれませんが、テスト時に使用が許可されているのはこれらのみです。トレーニング/開発中は、システムシミュレータが提供するあらゆる情報を自由に使用できます。

#### システム入力

システムは、ロボットをナビゲートするためにAIモジュールから出力されるウェイポイントを受け取ります。通行可能領域（上記参照）にあるウェイポイントは直接受け入れられ、通行可能領域外のウェイポイントは調整されて通行可能領域に移動されます。システムはまた、選択されたオブジェクトを強調表示するためのモジュールから出力される可視化マーカーも受け取ります。数値応答を示すInt32メッセージは、ロボットをナビゲートするためにシステムによって直接使用されることはなく、代わりに[評価](#評価)セクションで詳述される評価ノードによって読み取られます。

以下の表は、使用するROSトピックを示しています。ウェイポイントは指示追従質問に使用され、可視化マーカーはオブジェクト参照質問の出力であり、整数は数値質問用です。

| メッセージ | 説明 | ROSトピック名 |
|-|-|-|
| Waypoint with Heading | 位置と方向を持つROS Pose2Dメッセージ | `/way_point_with_heading` |
| Selected Object Marker | 選択されたオブジェクトのオブジェクトラベルとバウンディングボックスを含むROS Markerメッセージ | `/selected_object_marker` |
| Numerical Response | 数値質問への回答として整数を含むROS Int32メッセージ | `/numerical_response` |

物理システムで使用される座標フレームを以下に示します。LiDAR（センサーフレーム）に対するカメラ位置（カメラフレーム）は、CADモデルに基づいて測定されます。方向は較正され、画像はカメラフレームとLiDARフレームを整列させるために再マッピングされます。

<p align="center">
  <img src="figures/system.png" alt="system" width="30%"/>
</p>

### シミュレータ

上記のUnityベースのシミュレーションシステムに加えて、AI HabitatベースでMatterport3D環境モデルを使用する第二のシミュレータが[matterport/](system/matterport/)下に提供されています。同様に、システムを起動でき、合成データを取得でき、ウェイポイントを送信することでシミュレーション内でロボットをナビゲートします。簡単な例として、以下のコマンドラインは開始点から1m離れた場所にシステムへのウェイポイントを送信します。

`rostopic pub -1 /way_point_with_heading geometry_msgs/Pose2D '{x: 1.0, y: 0.0, theta: 0.0}'`

シミュレーションシステムは実際のチャレンジよりも広範囲のデータを提供しますが、データはAIモジュールの準備に役立つ可能性があることに注意してください。チャレンジ中は、上記の[システム](#システム)にリストされているデータのみが提供され、実ロボットのオンボードデータと一致します。

![image](figures/simulator.png)

### オブジェクト参照データセット (VLA-3D)

参照オブジェクトグラウンディングのサブタスクを支援するために、11K以上の領域と900万以上の文を持つ7.6Kの屋内3Dシーンを含むVLA-3Dデータセットが提供されています。データセットには、処理されたシーンポイントクラウド、オブジェクトと領域ラベル、意味的関係のシーングラフ、多様なデータソースからの各3Dシーンに対する生成された言語文が含まれており、Unity内の15のトレーニングシーンも含まれています。データへのアクセスとフォーマットの詳細については、[VLA-3Dリポジトリ](https://github.com/HaochenZ11/VLA-3D)をご覧ください。

## 実ロボットチャレンジ (2025)

2025年から、チャレンジ評価の最終ラウンドは実ロボットシステムで行われ、初期評価ラウンドは引き続きシミュレーションで実施されます。シミュレータと同様に、システムは以下に説明するオンボードデータを提供し、シミュレータと同じ方法でウェイポイントを受け取ります。AIモジュールで開発されたソフトウェアは、シーンを探索するためにウェイポイントを送信することしかできません。手動でウェイポイントを送信したり、遠隔操作したりすることは許可されていません。最終評価段階では、各チームはロボットのオンボードコンピューター（16x i9 CPUコア、32GB RAM、RTX 4090 GPU）にリモートログインし、自律モジュールとインターフェースを持つDockerコンテナ内でソフトウェアをセットアップします。Dockerコンテナは各チーム単独で使用され、他のチームと共有されません。シミュレーションラウンドをパスしたチームがソフトウェアをセットアップし、その段階でロボットをテストするためのタイムスロットをスケジュールします。チームはロボットのオンボードコンピューターでデータを記録することもでき、このデータは後で参加者に提供されます。

### 実ロボットデータ

実システムから収集されたサンプルシーンデータは、オブジェクトレイアウトにいくつかの違いがある[こちら](https://drive.google.com/drive/folders/1M0_UkY7aDIEpoVK6GdWzg45v-HX2UMSd?usp=drive_link)で提供されています。サンプルデータには以下が含まれています：

- `data_view.rviz`：データ表示用に提供されたRVIZ構成ファイル
- `map.ply`：オブジェクトセグメンテーションとIDを含むグラウンドトゥルースマップ
- `object_list.txt`：バウンディングボックスとラベルを含むオブジェクトリスト
- `system.zip`：チャレンジと同じ形式でシステムが提供するROSメッセージを含む圧縮されたバッグファイル
- `readme.txt`：較正情報とサンプルファイルに関する詳細

ここで、グラウンドトゥルースマップとオブジェクトリストはチャレンジ中に提供されるファイルではありませんが、システムから取得および処理できる情報のサンプルとして示されています。LiDAR（センサーフレーム）に対するカメラポーズ（カメラフレーム）は、含まれているREADMEファイルで見つけることができます。ファイルに関する詳細もREADMEテキストファイルに記載されています。


## 提出
提出は、公開リポジトリへのGitHubリポジトリリンクとして行われます。最も簡単な方法は、このリポジトリをフォークしてそこに変更を加えることです。提出されたリポジトリは同じ方法で構造化される必要があるためです。変更すべきファイル/フォルダは[ai_module](ai_module/)下のものと、場合によっては[launch.sh](launch.sh)のみです。パッケージをインストールするためにDockerイメージに変更が加えられた場合は、更新されたイメージを[Docker Hub](https://hub.docker.com/)にプッシュし、イメージへのリンクも提出してください。

提出前に、Dockerイメージをダウンロードしてシミュレータでテストしてください。提出物は同じ方法で評価されるためです。また、送信されたウェイポイントと可視化マーカーがサンプルダミーモデルのタイプと一致し、ベースナビゲーションシステムが正しく受信できるように同じROSトピック上にあることを確認してください。

GitHubリポジトリへのリンクを含む[提出フォーム](https://forms.gle/KsjYNaTzSTvvPafC9)に記入してください。


## 評価
提出されたコードはプルされ、リリースされたデータから保留された3つのUnity環境モデルで評価されます。各シーンは未知であり、モジュールには質問を探索して回答するための設定された時間があります（詳細は[タイミング](#タイミング)を参照）。テストシーンは、提供されたトレーニングシーンと同様のスタイルです。**以前にシーンを探索して収集した情報が保持されないように、テストされる各言語コマンドに対してシステムが再起動されます。**テスト時に使用が許可されているシステム上の情報は、[システム出力](#システム出力)にリストされているものに限定されることに注意してください。

評価は、ソースコードが公開されていない`challenge_evaluation_node`によって実行されます。評価ノードは、チーム提供のAIモジュールとシステムとともに同時に開始され、1Hzの頻度で以下のトピックにROS Stringメッセージとして各起動時に単一の質問を公開します：

| メッセージ | 説明 | 頻度 | ROSトピック名 |
|-|-|-|-|
| Challenge Question | 位置と方向を持つROS Pose2Dメッセージ | 1Hz | `/challenge_question` |

### 質問タイプと初期スコアリング

各シーンに対して、提供されたものと同様の5つの質問がテストされ、各応答にスコアが与えられます。質問タイプは次のようにスコア付けされます：
- **数値** (/1)：`/numerical_response`に`std_msgs/Int32`メッセージとして正確な数値を公開する必要があります。スコアは0または1。
- **オブジェクト参照** (/2)：ROS `visualization_msgs/Marker`メッセージを`/selected_object_marker`に公開する必要があり、グラウンドトゥルースオブジェクトバウンディングボックスとの重なり度合いに基づいてスコア付けされます。スコアは0から2の間。
- **指示追従** (/6)：車両をガイドするために、一連の`geometry_msgs/Pose2D`ウェイポイントを`/way_point_with_heading`に公開する必要があります。スコアは、コマンドの経路制約に従い、正しい順序で進むかどうかに基づいて、ロボットが実際に辿った軌跡に基づいて計算されます。辿った経路が正しい制約の順序から逸脱した場合、望ましい制約を達成しなかった場合、またはコマンドで禁止されている領域を通過した場合、スコアにペナルティが課されます。スコアは0から6の間で、部分点の可能性あり。

3つのテストシーンにわたるすべての質問からのスコアが各チームの最終スコアとして合計されます。

### タイミング

各質問に対して、システム起動時の再探索と質問への回答の両方が計時されます。タイミングはシステム起動時にすぐに開始されます。各質問には、テストシーンに関わらず、探索と質問への回答を合わせて**10分**の合計時間制限があります。特定の質問の時間制限を超えると、その質問に対して計算された初期スコアにペナルティが課されます。質問に割り当てられた時間よりも早く終了すると、その質問でボーナスポイントを獲得でき、これは同様の初期スコアを持つチーム間の同点を破るために使用されます。


## チャレンジFAQ
チャレンジに関する質問は、「question」ラベルでGitHub issueを開いて質問できます。チームの特定の状況に関する質問は、haochen4@andrew.cmu.eduまたは他のチャレンジ主催者にメールで送信できます。よくある質問はここに掲載されます。

1. 複数の提出は許可されていますか？

    はい、コンペティション中の提出回数に制限はありません。提出フォームは複数の提出を許可するように設定されており、最高スコアのものを採用します。

2. タスクを完了するための時間制約は何ですか？

    [タイミング](#タイミング)セクションをご確認ください。

3. LLMs/VLMs/APIの使用に制限はありますか？

    LLM、VLM、またはオンラインAPIの使用に制限はありません。どのモデルでも使用できますが、コードを実行できる必要があり、実行時にオンラインAPIにクエリする必要がある場合は、コードにアクセストークンを提供する必要があることに留意してください。

4. Dockerのサイズ制限は何ですか？

    サイズ制限は、評価を実行するために使用するマシンに依存します。マシンの仕様は[こちら](https://simplynuc.com/product/nuc13rngi9-full/?gad_source=1&gclid=CjwKCAjwiaa2BhAiE[%E2%80%A6]g4P7AnhLOZQVIoVC9croO7-i74DfuezIOztALzi5RVJ3jv3bxoCxmEQAvD_BwE)で確認できます。


## 参考文献

[1] J. Haas. "A history of the unity game engine," in Diss. Worcester Polytechnic Institute, vol. 483, no. 2014, pp. 484, 2014.